{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucas-Monteiro-Henriques/Implement_VISIONTS/blob/main/MAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdJj1mTcBy-"
      },
      "source": [
        "Importei MAE do Facebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxCYZD66YnCV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from scipy.signal import find_peaks\n",
        "from skimage.transform import resize\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "import torch\n",
        "import timm\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "np.float = float\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "17N5CayCl1Fj",
        "outputId": "67c68640-24fa-4f68-c126-30dbf83cf19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Collecting timm==0.4.5\n",
            "  Downloading timm-0.4.5-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.4.5) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.4.5) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.5) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.5) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.5) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.4.5) (3.0.2)\n",
            "Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.19\n",
            "    Uninstalling timm-1.0.19:\n",
            "      Successfully uninstalled timm-1.0.19\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-0.4.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "timm"
                ]
              },
              "id": "98662e2056c24cbc957426bf138cabb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mae'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Total 39 (delta 0), reused 0 (delta 0), pack-reused 39 (from 1)\u001b[K\n",
            "Receiving objects: 100% (39/39), 832.32 KiB | 9.68 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "repo_dir = 'mae'\n",
        "repo_url = 'https://github.com/facebookresearch/mae.git'\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.4.5  # 0.3.2 does not work in Colab\n",
        "    if not os.path.exists('mae'):\n",
        "        !git clone https://github.com/facebookresearch/mae.git\n",
        "    sys.path.append('./mae')\n",
        "else:\n",
        "    if not os.path.exists(repo_dir):\n",
        "        print(f'Cloned in : {repo_dir}')\n",
        "        subprocess.run(['git', 'clone', repo_url], check=True)\n",
        "    else:\n",
        "        print(f'Repository already exists in {repo_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8gGj4sIhYnCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a765b6ef-77a9-40f6-c08e-7757b2b4a74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-image-models'...\n",
            "remote: Enumerating objects: 20924, done.\u001b[K\n",
            "remote: Counting objects: 100% (371/371), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 20924 (delta 280), reused 213 (delta 212), pack-reused 20553 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20924/20924), 28.04 MiB | 19.03 MiB/s, done.\n",
            "Resolving deltas: 100% (15531/15531), done.\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists('pytorch_image_models'):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/huggingface/pytorch-image-models.git\n",
        "    os.rename('pytorch-image-models', 'pytorch_image_models') #renomeia a pasta porque dÃ¡ conflito nos imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ddNRyMfoYnCY"
      },
      "outputs": [],
      "source": [
        "# from pytorch_image_models.timm.models.vision_transformer import PatchEmbed, Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vl4fijyFYnCY"
      },
      "outputs": [],
      "source": [
        "mae_path = os.path.abspath(\"mae\")\n",
        "if mae_path not in sys.path:\n",
        "    sys.path.insert(0, mae_path)\n",
        "\n",
        "from models_mae import mae_vit_base_patch16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FbTP-0tfYnCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4ed887ed-8e72-47af-e6fd-a5011e0d436d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Block.__init__() got an unexpected keyword argument 'qk_scale'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-4142559647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae_vit_base_patch16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/mae/models_mae.py\u001b[0m in \u001b[0;36mmae_vit_base_patch16_dec512d8b\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmae_vit_base_patch16_dec512d8b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     model = MaskedAutoencoderViT(\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mdecoder_embed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_num_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mae/models_mae.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size, patch_size, in_chans, embed_dim, depth, num_heads, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, norm_layer, norm_pix_loss)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fixed sin-cos embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         self.blocks = nn.ModuleList([\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             for i in range(depth)])\n",
            "\u001b[0;32m/content/mae/models_mae.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         self.blocks = nn.ModuleList([\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             for i in range(depth)])\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Block.__init__() got an unexpected keyword argument 'qk_scale'"
          ]
        }
      ],
      "source": [
        "model = mae_vit_base_patch16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkWKZa2SIlZ"
      },
      "source": [
        "ImplementaÃ§Ã£o do Algoritmo de transformaÃ§Ã£o das sÃ©ries temporais 1D para 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s5FCRay9rHvw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://github.com/zhouhaoyi/ETDataset/raw/main/ETT-small/ETTm1.csv\")\n",
        "\n",
        "\n",
        "serie = df[\"OT\"].values[:512]\n",
        "\n",
        "acf_vals = acf(serie, nlags=min(50, len(serie)//2), fft=True)\n",
        "\n",
        "lags = np.arange(1, len(acf_vals))\n",
        "acf_sem_lag0 = acf_vals[1:]\n",
        "peaks, _ = find_peaks(acf_sem_lag0, height=0.2)\n",
        "\n",
        "if len(peaks) == 0:\n",
        "    P = 1\n",
        "else:\n",
        "    P = peaks[0] + 1\n",
        "\n",
        "print(f\"Periodicidade estimada: {P}\")\n",
        "\n",
        "L = (len(serie) // P) * P\n",
        "serie_cortada = serie[:L]\n",
        "\n",
        "\n",
        "matriz_2d = serie_cortada.reshape(P, L // P)\n",
        "print(\"Matriz 2D:\\n\", matriz_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6r3OzOrc2xn"
      },
      "source": [
        "Normalization do Iraw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1CPDb_UUBVR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from statistics import mean\n",
        "desvio_padrao = np.std(matriz_2d)\n",
        "print(desvio_padrao)\n",
        "mean  = np.mean(matriz_2d)\n",
        "print(mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jyKVebIsS6Hs"
      },
      "outputs": [],
      "source": [
        "r = 0.4\n",
        "Inorm = r * (matriz_2d - mean)/ desvio_padrao\n",
        "print(Inorm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwigDzq7eHF6"
      },
      "source": [
        "Esse Ã© o algorimo do Alignment, mas nÃ£o foi testado, pois o MAE do Facebook precisa ser alterado na parte da mascara\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TTUSyf8uY_Vo"
      },
      "outputs": [],
      "source": [
        "L = 300\n",
        "H = 100\n",
        "c = 0.4\n",
        "S = 1\n",
        "N = max(Inorm.shape)\n",
        "\n",
        "# Etapa 1: Render Igray (imagem 3 canais iguais)\n",
        "Igray = np.stack([Inorm] * 3, axis=-1)\n",
        "\n",
        "# Etapa 2: Alignment\n",
        "n = int(np.floor(c * N * L / (L + H)))  # nÃºmero de patches visÃ­veis na horizontal\n",
        "target_shape = (N * S, n * S)           # shape final da imagem apÃ³s resize\n",
        "\n",
        "# Resize Igray (mantendo apenas 1 canal para simplificar o MAE)\n",
        "Igray_resized = resize(Inorm, target_shape, order=1, mode='reflect', anti_aliasing=False)\n",
        "\n",
        "# Converter para imagem 3 canais (opcional, dependendo do input do MAE)\n",
        "Igray_resized_3ch = np.stack([Igray_resized] * 3, axis=-1)\n",
        "\n",
        "# VisualizaÃ§Ã£o\n",
        "plt.imshow(Igray_resized, cmap='gray', aspect='auto')\n",
        "plt.title(\"Igray redimensionado para Alignment (VisionTS)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Print das formas para verificaÃ§Ã£o\n",
        "print(\"Shape original Inorm:\", Inorm.shape)\n",
        "print(\"Shape Igray:\", Igray.shape)\n",
        "print(\"Shape Igray_resized:\", Igray_resized.shape)\n",
        "print(\"Shape Igray_resized_3ch:\", Igray_resized_3ch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjjNqcc_cG5n"
      },
      "source": [
        "FunÃ§Ãµes do MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5koUFBqPirM-"
      },
      "outputs": [],
      "source": [
        "# define the utils\n",
        "\n",
        "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
        "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def show_image(image, title=''):\n",
        "    # image is [H, W, 3]\n",
        "    assert image.shape[2] == 3\n",
        "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    return\n",
        "\n",
        "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
        "    # build model\n",
        "    from models_mae import mae_vit_base_patch16\n",
        "    model = mae_vit_base_patch16()\n",
        "\n",
        "    # load model\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
        "    print(msg)\n",
        "    return model\n",
        "\n",
        "def run_one_image_from_aligned_image(Igray_resized_3ch, model, L=300, H=100, c=0.4):\n",
        "    \"\"\"\n",
        "    Igray_resized_3ch: imagem com 3 canais (jÃ¡ alinhada), shape (H, W, 3), valores jÃ¡ normalizados\n",
        "    model: modelo MAE do repo da Meta (Vit-base etc)\n",
        "    L, H, c: hiperparÃ¢metros do VisionTS\n",
        "    \"\"\"\n",
        "    # 1. Redimensionar para 224x224 (como esperado pelo MAE)\n",
        "    img = resize(Igray_resized_3ch, (224, 224), order=1, mode='reflect', anti_aliasing=False)\n",
        "    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0)       # [1, H, W, C]\n",
        "    if False:\n",
        "        x = torch.einsum('nhwc->nchw', x).cuda()                      # [1, 3, 224, 224]\n",
        "    if True:\n",
        "        x = torch.einsum('nhwc->nchw', x).to(device)\n",
        "\n",
        "    # 2. Criar mÃ¡scara estruturada: lado direito mascarado\n",
        "    patch_size = model.patch_embed.patch_size[0]         # geralmente 16\n",
        "    num_patches_row = x.shape[2] // patch_size           # 224 / 16 = 14\n",
        "    num_patches = num_patches_row ** 2                   # 14 x 14 = 196\n",
        "\n",
        "    N = num_patches_row\n",
        "    n = int(np.floor(c * N * L / (L + H)))                # nÃºmero de colunas visÃ­veis\n",
        "\n",
        "    # Criar mÃ¡scara: 0 = visÃ­vel (esquerda), 1 = mascarado (direita)\n",
        "    mask = torch.ones(num_patches, device=x.device)\n",
        "    for row in range(N):\n",
        "        for col in range(n):  # colunas visÃ­veis\n",
        "            idx = row * N + col\n",
        "            mask[idx] = 0\n",
        "    mask = mask.unsqueeze(0)  # [1, num_patches]\n",
        "\n",
        "    # 3. Forward com mÃ¡scara estruturada\n",
        "    with torch.no_grad():\n",
        "        if False:\n",
        "            loss, y, _ = model(x.float(), mask=mask)         # usamos mask diretamente\n",
        "        if True:\n",
        "            loss, y, _ = model(x.float())\n",
        "        y = model.unpatchify(y)                          # [1, 3, H, W]\n",
        "        if False:\n",
        "            y = torch.einsum('nchw->nhwc', y).cpu().numpy()\n",
        "        if True:\n",
        "            y = torch.einsum('nchw->nhwc', y).cpu()\n",
        "\n",
        "        mask_recon = mask.unsqueeze(-1).repeat(1, 1, patch_size ** 2 * 3)\n",
        "        mask_recon = model.unpatchify(mask_recon)\n",
        "        mask_recon = torch.einsum('nchw->nhwc', mask_recon).cpu()\n",
        "\n",
        "    x_vis = x.cpu()\n",
        "    x_vis = torch.einsum('nchw->nhwc', x_vis)\n",
        "\n",
        "    im_masked = x_vis * (1 - mask_recon)\n",
        "    im_paste = x_vis * (1 - mask_recon) + y * mask_recon\n",
        "\n",
        "    # 4. VisualizaÃ§Ã£o\n",
        "    plt.rcParams['figure.figsize'] = [24, 6]\n",
        "    plt.subplot(1, 4, 1); show_image(x_vis[0], \"original\")\n",
        "    plt.subplot(1, 4, 2); show_image(im_masked[0], \"structured mask\")\n",
        "    plt.subplot(1, 4, 3); show_image(y[0], \"reconstruction\")\n",
        "    plt.subplot(1, 4, 4); show_image(im_paste[0], \"reconstruction + visible\")\n",
        "    plt.show()\n",
        "\n",
        "    return y[0]  # retorno da imagem reconstruÃ­da (H, W, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMhEYP-icP3_"
      },
      "source": [
        "VizulizaÃ§Ã£o da imagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "00z2PJZ5iyYJ"
      },
      "outputs": [],
      "source": [
        "# Suponha que vocÃª jÃ¡ tem:\n",
        "# Igray_resized_3ch com shape (29, 8, 3) â€” imagem com 3 canais e normalizada (VisionTS-style)\n",
        "\n",
        "# Etapa 1: Redimensionar para 224 x 224 (como esperado pelo MAE)\n",
        "img = resize(Igray_resized_3ch, (224, 224), order=1, mode='reflect', anti_aliasing=False)\n",
        "\n",
        "# Etapa 2: VisualizaÃ§Ã£o opcional (sem normalizar com ImageNet)\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "plt.imshow(img)\n",
        "plt.title(\"Input aligned (VisionTS)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Se for usar a funÃ§Ã£o run_one_image, ela espera um tensor com valores normalizados jÃ¡\n",
        "# Portanto vocÃª pode usar:\n",
        "img = torch.tensor(img, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPM66Pu1cY8z"
      },
      "source": [
        "Download dos pesos de Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zct5FlqgpzmX"
      },
      "outputs": [],
      "source": [
        "# MAE com decoder\n",
        "if False:\n",
        "    !wget -nc https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_base.pth\n",
        "\n",
        "if True:\n",
        "    url = 'https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_base.pth'\n",
        "    dest = 'mae_visualize_vit_base.pth'\n",
        "\n",
        "    if not os.path.exists(dest):\n",
        "        print('Downloading MAE with decoder...')\n",
        "        urllib.request.urlretrieve(url, dest)\n",
        "    else:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r2Qq8iJvi4dO"
      },
      "outputs": [],
      "source": [
        "# Carrega o modelo base\n",
        "\n",
        "from models_mae import mae_vit_base_patch16\n",
        "\n",
        "chkpt_dir = 'mae_visualize_vit_base.pth'\n",
        "model = mae_vit_base_patch16()\n",
        "checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
        "print(msg)\n",
        "\n",
        "# Enviar para GPU\n",
        "model.eval()\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JmNiufmZi8Uv"
      },
      "outputs": [],
      "source": [
        "# make random mask reproducible (comment out to make it change)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # usa cuda ou cpu\n",
        "model = model.to(device)\n",
        "\n",
        "# Garante reprodutibilidade na mÃ¡scara aleatÃ³ria\n",
        "torch.manual_seed(2)\n",
        "\n",
        "Igray_resized_3ch_tensor = torch.from_numpy(Igray_resized_3ch).float().to(device)\n",
        "print('MAE with pixel reconstruction:')\n",
        "reconstructed_image = run_one_image_from_aligned_image(Igray_resized_3ch, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62NdTXsepgF"
      },
      "source": [
        "Algoritmo de reconstruÃ§Ã£o, mudar o L e H  em def reconstruct_and_evaluate_forecast para\n",
        "Tabela oficial dos resultados (extraÃ­da do artigo)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“Š Tabela oficial dos resultados no dataset ETTm1:\n",
        "# -----------------------------------------------------------\n",
        "# | L   | H   | MSE â†“   | MAE â†“   |\n",
        "# |-----|-----|---------|---------|\n",
        "# | 96  | 96  | 0.234   | 0.368   |\n",
        "# | 96  | 192 | 0.286   | 0.411   |\n",
        "# | 96  | 336 | 0.287   | 0.414   |\n",
        "# | 96  | 720 | 0.365   | 0.470   |\n",
        "# -----------------------------------------------------------\n",
        "#"
      ],
      "metadata": {
        "id": "RvekcjlmKTUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqhQUHz6_o1A"
      },
      "outputs": [],
      "source": [
        "def reconstruct_and_evaluate_forecast(y_reconstructed, original_Inorm):\n",
        "    \"\"\"\n",
        "    y_reconstructed: imagem reconstruÃ­da pelo MAE, shape (H, W, 3)\n",
        "    original_Inorm: matriz 2D usada originalmente, shape (P, L/P)\n",
        "    r: fator de normalizaÃ§Ã£o aplicado\n",
        "    \"\"\"\n",
        "    r = 0.4\n",
        "    import numpy as np\n",
        "    from skimage.transform import resize\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "    # Etapa 1: mÃ©dia dos 3 canais para obter imagem grayscale reconstruÃ­da\n",
        "    recon_gray = y_reconstructed.mean(axis=-1)  # shape (H, W)\n",
        "\n",
        "    # Etapa 2: resize reverso para shape original\n",
        "    recon_resized_back = resize(recon_gray, original_Inorm.shape, order=1, mode='reflect', anti_aliasing=False)\n",
        "\n",
        "    # Etapa 3: flatten\n",
        "    recon_flat = recon_resized_back.flatten()\n",
        "    orig_flat = original_Inorm.flatten()\n",
        "\n",
        "    # Etapa 4: desnormalizar\n",
        "    mean_orig = np.mean(orig_flat / r)\n",
        "    std_orig = np.std(orig_flat / r)\n",
        "    recon_deno = recon_flat / r\n",
        "    recon_deno = recon_deno * std_orig + mean_orig\n",
        "\n",
        "    # Etapa 5: extrair a janela de previsÃ£o\n",
        "    P, total_steps = original_Inorm.shape\n",
        "    H = 96\n",
        "    L = 96\n",
        "    pred_steps = int(0.4 * L * total_steps / (L + H))\n",
        "    forecast_start = total_steps - pred_steps\n",
        "\n",
        "    recon_forecast = recon_deno.reshape(P, total_steps)[:, forecast_start:]\n",
        "    target_forecast = (original_Inorm / r).reshape(P, total_steps)[:, forecast_start:]\n",
        "    target_forecast = target_forecast * std_orig + mean_orig\n",
        "\n",
        "    mse = mean_squared_error(target_forecast.flatten(), recon_forecast.flatten())\n",
        "    mae = mean_absolute_error(target_forecast.flatten(), recon_forecast.flatten())\n",
        "\n",
        "    print(f\"MSE na janela de previsÃ£o: {mse:.6f}\")\n",
        "    print(f\"MAE na janela de previsÃ£o: {mae:.6f}\")\n",
        "\n",
        "    return recon_forecast, target_forecast, mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kHqgxTc4hZjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "recon_forecast, target_forecast, mse = reconstruct_and_evaluate_forecast(\n",
        "    y_reconstructed=reconstructed_image.numpy(),\n",
        "    original_Inorm=Inorm\n",
        ")\n"
      ],
      "metadata": {
        "id": "E-FPmLmMd3nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(target_forecast.flatten(), label='target', color='gray')\n",
        "plt.plot(recon_forecast.flatten(), label='prediction', color='steelblue')\n",
        "plt.title(f\"Forecast (Ãºltimos {target_forecast.shape[1]} steps) - MSE: {mse:.4f}\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MS715FvgeQkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}